---
layout:     post
title:      强化学习基础--（1）
subtitle:   强化学习的基本概念
date:       2021-3-1
author:     BY
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - RL
---

# RL1

强化学习同监督学习的难点：

1. 数据不满足i.i.d.，连续性，依赖性
2. 不能立刻得到反馈，延迟奖励
3. 输入的是序列数据

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222142515072.png" alt="image-20210222142515072" style="zoom:50%;" />



![image-20210222154259401](C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222154259401.png)



![image-20210222142628250](C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222142628250.png)



<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222142654860.png" alt="image-20210222142654860" style="zoom:25%;" />

sequential decision making

![image-20210222154320287](C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222154320287.png)

![image-20210222143014304](C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222143014304.png)

注意几个概念的区分

![image-20210222143124017](C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222143124017.png)



<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222151502627.png" alt="image-20210222151502627" style="zoom:25%;" />

另外注意到和环境中往往有可能有很多随机因素。`randomness`

![image-20210222152040249](C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222152040249.png)

一个agent的不同组成部分:

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222144414340.png" alt="image-20210222144414340" style="zoom:25%;" />

注意图里在st做at产生的reward是rt+1，看的是未来。也就是Ht=O1,R1,A1->O2,R2,A2->...->At-1,Ot,Rt

1. policy function`,两种策略，
   1. `stochastic function`,$\pi(s|a) = P[A_t=a|S_t=s]$,根据状态s生成一个概率分布，再对这个概率分布采样来完成action，在学习时可以引入**随机性来做exploration**,而且随机性策略的动作**具有多样性**，这一点在多个智能体博弈时也非常重要。采用确定性策略的智能体总是对同样的环境做出相同的动作，会导致它的策略很容易被对手预测。
   2. `deterministic function`，$a^*=argmax \pi(a|s)$,极大化
2. `Value function`，作为对**未来奖励**的预测评估**状态的好坏**，并且我们希望短时间内获得更多的利益，所以引入`discount factor`作为惩罚因子。这个 π 函数就是说在我们**已知某一个策略函数的时候，到底可以得到多少的奖励。**
   1. 作为**状态**的函数的期望表示,$v_{\pi}(s)=E_\pi[G_t|S_t=s]=E_\pi[\sum_{k=0}^\infin \gamma^kR_{t+k+1}|S_t=s],for all s \in S$,注意到评估的是，当前的状态t，对未来的时间步的奖励函数的期望值:t+1,t+2,t+3，并且引入了惩罚因子，因为我们希望更快获得更多的奖励。
   2. 另一种Q函数，同时评估**状态和动作**,$q_\pi(s,a)=E_\pi[\sum_{k=0}^\infin \gamma^k R_{t+k+1}|S_t=s,A_t=a]$,因为当我们得到这个 Q 函数后，进入某一种状态，它最优的行为就可以通过这个 Q 函数来得到。
3. `Model`,**模型决定了下一个状态会是什么样的，就是说下一步的状态取决于你当前的状态以及你当前采取的行为。**

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222145039620.png" alt="image-20210222145039620" style="zoom:25%;" />

三个部分合起来构成了一个`MDP`。

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222145202694.png" alt="image-20210222145202694" style="zoom:25%;" />

从一个S出发不同的a，实际上就是`policy function`，而过渡到另一个状态和获得的奖励，就是`Model`(当然也有Model based与Model free的区别，后者不需要agent模拟环境，而是直接与环境互动)，而`value function`则评估当前状态的好坏。

![image-20210222154449307](C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222154449307.png)

来看两种不同的RL。

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222145459841.png" alt="image-20210222145459841" style="zoom:25%;" />



<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222145650009.png" alt="image-20210222145650009" style="zoom:25%;" />

根据agent学习东西的不同，我们可以对RL进行归类

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222145952702.png" alt="image-20210222145952702" style="zoom:33%;" />

另一种归类方式是是否依赖与真实环境作互动

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222150109017.png" alt="image-20210222150109017" style="zoom:33%;" />

然而在实际应用中，智能体并不是那么容易就能知晓 MDP 中的所有元素的。**通常情况下，状态转移函数和奖励函数很难估计，甚至连环境中的状态都可能是未知的，这时就需要采用免模型学习。**免模型学习没有对真实环境进行建模，智能体只能在真实环境中通过一定的策略来执行动作，等待奖励和状态迁移，然后根据这些反馈信息来更新行为策略，这样反复迭代直到学习到最优策略。

在实际应用中，如果不清楚该用有模型强化学习还是免模型强化学习，可以先思考一下，**在智能体执行动作前，是否能对下一步的状态和奖励进行预测，**如果可以，就能够对环境进行建模，从而采用有模型学习。有模型学习相比于免模型学习仅仅多出一个步骤，一些有模型的强化学习方法，也可以在免模型的强化学习方法中使用。

免模型学习通常属于**数据驱动型方法，需要大量的采样**来估计状态、动作及奖励函数，从而优化动作策略。相比之下，有模型学习可以在一定程度上**缓解训练数据匮乏**的问题，因为智能体可以在虚拟世界中行训练。

**免模型学习的泛化性要优于有模型学习**，原因是有模型学习算需要对真实环境进行建模，并且虚拟世界与真实环境之间可能还有差异，这限制了有模型学习算法的泛化性。

有模型的强化学习方法可以**对环境建模**，使得该类方法具有独特魅力，**即“想象能力”**。在免模型学习中，智能体只能一步一步地采取策略，等待真实环境的反馈；而有模型学习可以**在虚拟世界中预测出所有将要发生的事，并采取对自己最有利的策略。**

![image-20210222150451279](C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222150451279.png)

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222150544091.png" alt="image-20210222150544091" style="zoom:25%;" />

**Learning**与**Planning**是RL的两个基本问题：

+ 在强化学习中，环境初始时是未知的，agent 不知道环境如何工作，agent 通过不断地与环境交互，逐渐改进策略。<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222150747357.png" alt="image-20210222150747357" style="zoom:25%;" />

+ <img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222150844235.png" alt="image-20210222150844235" style="zoom:25%;" />

+ planning的思路实际上就是model-based的想法。
+ <img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222151018517.png" alt="image-20210222151018517" style="zoom:25%;" />



<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222151259572.png" alt="image-20210222151259572" style="zoom:25%;" />

# 2.MDP

马尔可夫决策过程是强化学习里面的一个基本框架,agent与environment作互动，它的环境是全部可以观测的(`fully observable`)。但是很多时候环境里面有些量是不可观测的，但是这个部分观测的问题也可以转换成一个 MDP 的问题。

所谓马尔可夫性质，就是下一步状态只取决于当前状态，与当前状态之前的状态都没有关系。

![image-20210222154726283](C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222154726283.png)

![image-20210222154748581](C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222154748581.png)

在马尔科夫链中引入奖励函数，就得到了马尔可夫奖励过程`MRP`，多了一个`奖励函数(reward function)`。**奖励函数是一个期望**，就是说当你到达某一个状态的时候，可以获得多大的奖励，然后这里另外定义了一个 `discount factor γ`

![image-20210222155855981](C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222155855981.png)

我们通过一个形象的例子来理解 MRP。我们把一个纸船放到河流之中，那么它就会随着这个河流而流动，它自身是没有动力的。所以你可以把 MRP  看成是一个随波逐流的例子，当我们从某一个点开始的时候，这个纸船就会随着事先定义好的状态转移进行流动，它到达每个状态过后，我们就有可能获得一些奖励。

比如通过向量引入一个简单的奖励函数$R=[5,0,0,0,0,0,10]$，代表到达不同点时获得的奖励。注意是**期望**。注意是有限步数。

![image-20210222155812536](C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222155812536.png)

于是一个简单的问题是，我们如何对于一个给定的状态计算其价值呢？一个可行的做法是采样不同的轨迹，用平均值估计期望值，这就是所谓的**蒙特卡洛算法**。

另一种计算方法是使用`Bellman equantion`,其定义了状态之间的迭代关系。

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222160337456.png" alt="image-20210222160337456" />

![image-20210222160809471](C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222160809471.png)



<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222160818419.png" alt="image-20210222160818419" style="zoom:25%;" />

注意此处的rt+1代表st的奖励，不知道为啥这么标？因为是看向未来的。

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222162134000.png" alt="image-20210222162134000" style="zoom:25%;" />



![image-20210222162401984](C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222162401984.png)

另一种方法是用**蒙特卡洛模拟**，或者用bellman interactive。

![image-20210222190512664](C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222190512664.png)



![image-20210222190532240](C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222190532240.png)

而在MRP中加上decision，就得到了`MDP`。

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222162740831.png" alt="image-20210222162740831" />

**这里说明了 MDP 跟 MRP 的之间的一个转换。**已知一个 MDP 和一个 policy π\piπ 的时候，我们可以把 MDP 转换成 MRP。在 MDP 里面，转移函数 P(s′∣s,a)P(s'|s,a)P(s′∣s,a)  是基于它当前状态以及它当前的 action。因为我们现在已知它 policy  function，就是说在每一个状态，我们知道它可能采取的动作的概率，那么就可以直接把这个 action 进行加和，直接把这个 a  去掉，那我们就可以得到对于 MRP 的一个转移，这里就没有 action。对于这个奖励函数，我们也可以把 action  拿掉，这样就会得到一个类似于 MRP 的奖励函数。

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222163145373.png" alt="image-20210222163145373" style="zoom:33%;" />

同理我们可以定义MDP中的`state value function`,注意到这里是对一个给定的policy计算的价值函数。

![image-20210222163625542](C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222163625542.png)

Q函数也具有bellman-equantion的性质。注意以下几个等式用于Interactive learning。蕴含了backup。

![image-20210222190644151](C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222190644151.png)

根据等式，可以通过bellman update来寻找Vpi。

![image-20210222164456600](C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222164456600.png)

MDP是满足动态规划的两个性质的，最优子问题与重叠子结构

![image-20210222191026232](C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222191026232.png)

如下显式怎么做back up

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222191229667.png" alt="image-20210222191229667" style="zoom:25%;" />

注意到bellman equation说明的是当前状态s与未来状态s'之间的关系，我们可以根据未来状态向当前状态backup，而在更新中的下标t则代表每一步更新的value function，我们最终就要找到每个状态收敛了的价值函数的值。

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222191724310.png" alt="image-20210222191724310" style="zoom:33%;" />

因为已经给定了这个函数的 policy  function，那我们可以直接把它简化成一个 MRP 的表达形式，这样的话，形式就更简洁一些，**就相当于我们把这个 a 去掉**，这样它就**只有价值函数跟转移函数**了。通过去迭代这个更简化的一个函数，我们也可以得到它每个状态的价值。**因为不管是在 MRP 以及 MDP，它的价值函数包含的这个变量都是只跟这个状态有关，就相当于进入某一个状态，未来可能得到多大的价值。**

**policy evaluation/prediction**是对于给定策略计算value function的方法，用bellman update就好了。

另一个是所谓**policy control**，需要找到最优策略与最优value function，也就是`optimal value function`：$v^*(s)=max_\pi v^\pi(s)$,找一种策略使得每个状态的value最大。这也就是说所谓的`optimal policy`是指:$\pi^*(s)=argmax_\pi v^\pi(s)$。

Optimal policy 使得每个状态的价值函数都取得最大值。所以如果我们可以得到一个 optimal value  function，就可以说某一个 MDP  的环境被解。在这种情况下，它的最佳的价值函数是一致的，就它达到的这个上限的值是一致的，但这里可能有多个最佳的 policy，就是说多个  policy 可以取得相同的最佳价值。

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222193217924.png" alt="image-20210222193217924" style="zoom:33%;" />

一种方法是如果我们能优化出Q函数，就可以直接得到最优的policy与value function。

最简单的搜索方法就是穷举。然后计算所有的value function对比以下即可。但效率太低了。

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222193437775.png" alt="image-20210222193437775" style="zoom:25%;" />

对于一个事先定好的 MDP 过程，当 agent 去采取最佳策略的时候，我们可以说最佳策略一般都是确定的，而且是稳定的(它不会随着时间的变化)。但是不一定是唯一的，多种动作可能会取得相同的这个价值。

一种方法是**policy interaction**，先用当前的pi计算一个value function,再由此计算Q函数，最后直接在Q函数上贪心的更新pi就好了。

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222194159888.png" alt="image-20210222194159888" style="zoom:25%;" />

接着就是对Q-table取argmax就可以了。

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222194740406.png" alt="image-20210222194740406" style="zoom:25%;" />

我们需要简单说明一下为什么这种方法可以得到最优解，事实上，每次更新都是单调的。不断利用不等式4再加上bellman expection就行了。

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222195534177.png" alt="image-20210222195534177" style="zoom:25%;" />

最终收敛的结果就得到了最优策略。

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222195942272.png" alt="image-20210222195942272" style="zoom:25%;" />

bellman optimality equation说明了，最佳策略下的一个状态的价值必须等于在这个状态下采取最好动作得到的回报的期望。 

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222200123660.png" alt="image-20210222200123660" style="zoom:25%;" />

由此得到了最优状态下状态价值函数的转移。

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222200505919.png" alt="image-20210222200505919" style="zoom:33%;" />

另一种方法是所谓的**value interaction**，**Value iteration 就是把 Bellman Optimality Equation 当成一个 update rule 来进行**

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222200557838.png" alt="image-20210222200557838" style="zoom:25%;" />



<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222200800118.png" alt="image-20210222200800118" style="zoom:25%;" />

注意到我们只是在解决一个 planning 的问题，而不是强化学习的问题，因为我们知道环境如何变化。

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222201010746.png" alt="image-20210222201010746" style="zoom:25%;" />

最后我们来对比一下这两种方法。

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222201541069.png" alt="image-20210222201541069" style="zoom:25%;" />



<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222201556359.png" alt="image-20210222201556359" style="zoom:25%;" />



![image-20210222202006849](C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222202006849.png)

![image-20210222202023575](C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222202023575.png)

# 3.表格型方法

强化学习MDP的四元组<S,A,P,R>

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222203451030.png" alt="image-20210222203451030" style="zoom:33%;" />

**当我们知道 P 函数和  R 函数时，我们就说这个 MDP 是已知的，可以通过 policy iteration 和 value iteration 来找最佳的策略。**

但很多强化学习的经典算法都是 model-free 的，就是环境是未知的。

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222203807817.png" alt="image-20210222203807817" style="zoom:33%;" />

我们用价值函数 V(s)V(s)V(s) 来代表这个状态是好的还是坏的。

用 Q 函数来判断说在什么状态下做什么动作能够拿到最大奖励，用 Q 函数来表示这个状态-动作值。

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222203931331.png" alt="image-20210222203931331" style="zoom:25%;" />



<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222204021845.png" alt="image-20210222204021845" style="zoom:25%;" />

这里我们可以把RL看作是学习一个Q函数，实际上在离散状态与离散动作中，Q函数实际上就是一个Q-table。Q函数指导未来每一个动作的选择，最大化未来的总收益。

强化学习需要去学习远期的收益，因为在现实世界中奖励往往是延迟的。所以我们一般会从当前状态开始，把后续有可能会收到所有收益加起来计算当前动作的 Q 的价值，让 Q 的价值可以真正地代表当前这个状态下，动作的真正的价值。

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222204314675.png" alt="image-20210222204314675" style="zoom:25%;" />

![image-20210222204709565](C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222204709565.png)

最开始这张 Q 表格会全部初始化为零，然后 agent 会不断地去和环境交互得到不同的轨迹，当交互的次数足够多的时候，我们就可以估算出每一个状态下，每个行动的平均总收益去更新这个 Q  表格。怎么去更新 Q 表格就是接下来要引入的强化概念。

**`强化`就是我们可以用下一个状态的价值来更新当前状态的价值，其实就是强化学习里面 bootstrapping 的概念。**在强化学习里面，你可以每走一步更新一下 Q 表格，然后用下一个状态的 Q 值来更新这个状态的 Q 值，这种单步更新的方法叫做`时序差分`。

一般我们有两种方法来在model free的情况下估计某个给定策略的价值(evaluation)，**MC或者TD**。

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222205401316.png" alt="image-20210222205401316" style="zoom:33%;" />



<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222205521743.png" alt="image-20210222205521743" style="zoom:33%;" />

注意到此处的MC evaluation用的是`empirical mean return`的方法作估计，进一步我们可以推导出上一时刻均值与这一时刻均值的关系。

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222210001545.png" alt="image-20210222210001545" style="zoom:25%;" />

这样就可以每次得到一个新的trajectory就更新一次，这就是所谓的**incremental MC updates**

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222210259034.png" alt="image-20210222210259034" style="zoom:50%;" />



<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222210316106.png" alt="image-20210222210316106" style="zoom:33%;" />

我们可以直接把 $1N(St)\frac{1}{N(S_t)}N(St)1$ 变成 α\alphaα (学习率)，α\alphaα 代表着更新的速率有多快，我们可以进行设置。

![image-20210222210918722](C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222210918722.png)

而对于MC，只考虑采样到的state。

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222211547479.png" alt="image-20210222211547479" style="zoom:25%;" />



<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222211640192.png" alt="image-20210222211640192" style="zoom:33%;" />

另一种Model free的evaluation方法是`TD（时序差分）`

**`强化`就是我们可以用下一个状态的价值来更新当前状态的价值，其实就是强化学习里面 bootstrapping 的概念。**在强化学习里面，你可以每走一步更新一下 Q 表格，然后用下一个状态的 Q 值来更新这个状态的 Q 值，这种单步更新的方法叫做`时序差分`。

盆里的肉可以认为是强化学习里面那个延迟的 reward，声音的刺激可以认为是有 reward  的那个状态之前的一个状态。多次重复实验之后，最后的这个 reward  会强化小狗对于这个声音的条件反射，它会让小狗知道这个声音代表着有食物，这个声音对于小狗来说也就有了价值，它听到这个声音也会流口水。

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222212559339.png" alt="image-20210222212559339" style="zoom:25%;" />

![image-20210222212808814](C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222212808814.png)

- TD 是介于 MC 和 DP 之间的方法。
- TD 是 model-free 的，不需要 MDP 的转移矩阵和奖励函数。
- TD 可以从**不完整的** episode 中学习，结合了 bootstrapping 的思想。

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222212849016.png" alt="image-20210222212849016" style="zoom:25%;" />

我们来看一下TD的更新方式。

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222213856876.png" alt="image-20210222213856876" style="zoom:25%;" />

我们对比下 MC 和 TD：

- 在 MC 里面 Gi,tG_{i,t}Gi,t 是实际得到的值（可以看成 target），因为它已经把一条轨迹跑完了，可以算每个状态实际的 return。
- TD 没有等轨迹结束，往前走了一步，就可以更新价值函数。 
- TD 只执行了一步，状态的值就更新。
- MC 全部走完了之后，到了终止状态之后，再更新它的值。

接下来，进一步比较下 TD 和 MC。

- TD 可以在线学习(online learning)，每走一步就可以更新，效率高。
- MC 必须等游戏结束才可以学习。
- TD 可以从不完整序列上进行学习。
- MC 只能从完整的序列上进行学习。
- TD 可以在连续的环境下（没有终止）进行学习。
- MC 只能在有终止的情况下学习。
- TD 利用了马尔可夫性质，在马尔可夫环境下有更高的学习效率。
- MC 没有假设环境具有马尔可夫性质，利用采样的价值来估计某一个状态的价值，在不是马尔可夫的环境下更加有效。

**举个例子来解释 TD 和 MC 的区别，**

- TD 是指在不清楚马尔可夫状态转移概率的情况下，以采样的方式得到不完整的状态序列，估计某状态在该状态序列完整后可能得到的收益，并通过不断地采样持续更新价值。
- MC 则需要经历完整的状态序列后，再来更新状态的真实价值。

例如，你想获得开车去公司的时间，每天上班开车的经历就是一次采样。假设今天在路口 A 遇到了堵车，

- TD 会在路口 A 就开始更新预计到达路口 B、路口 C ⋯⋯\cdots \cdots⋯⋯，以及到达公司的时间；
- 而 MC 并不会立即更新时间，而是在到达公司后，再修改到达每个路口和公司的时间。

**TD 能够在知道结果之前就开始学习，相比 MC，其更快速、灵活。**

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222214500399.png" alt="image-20210222214500399" style="zoom:25%;" />



<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222214650015.png" alt="image-20210222214650015" style="zoom:25%;" />

进一步，可以推广到n-steps TD。

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222214755350.png" alt="image-20210222214755350" style="zoom:25%;" />

- Bootstrapping：更新时使用了估计：
  - MC 没用 bootstrapping，因为它是根据实际的 return 来更新。
  - DP 用了 bootstrapping。
  - TD 用了 bootstrapping。
- Sampling：更新时通过采样得到一个期望：
  - MC 是纯 sampling 的方法。
  - DP 没有用 sampling，它是直接用 Bellman expectation equation 来更新状态价值的。
  - TD 用了 sampling。TD target 由两部分组成，一部分是 sampling，一部分是 bootstrapping。

+ DP 是直接算 expectation，把它所有相关的状态都进行加和。

+ MC 在当前状态下，采一个支路，在一个path 上进行更新，更新这个 path 上的所有状态。

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222220041952.png" alt="image-20210222220041952" style="zoom:25%;" />



<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222220111492.png" alt="image-20210222220111492" style="zoom:25%;" />

以上是关于model-free evaluation的问题。

另一个问题是，如何寻找最优策略和最优价值函数，也就是所谓的**model-free control**问题。主要问题在于这时候我们不知道MDP的转移函数和奖励函数，需要把 policy iteration 进行一个广义的推广，使它能够兼容 MC 和 TD 的方法，即 `Generalized Policy Iteration(GPI) with MC and TD`。

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222220800178.png" alt="image-20210222220800178" style="zoom:25%;" />

针对上述情况，我们引入了广义的 policy iteration 的方法。

我们对 policy evaluation 部分进行修改：用 MC 的方法代替 DP 的方法去估计 Q 函数。

当得到 Q 函数后，就可以通过 greedy 的方法去改进它。

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222221512843.png" alt="image-20210222221512843" style="zoom:33%;" />

为了确保 MC 方法能够有足够的探索，我们使用了 ε\varepsilonε-greedy exploration。

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222221616148.png" alt="image-20210222221616148" style="zoom:33%;" />

当我们使用 MC 和 ε\varepsilonε-greedy 探索这个形式的时候，我们可以确保价值函数是单调的，改进的。

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222221817274.png" alt="image-20210222221817274" style="zoom:33%;" />

最终的算法是这样的

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222221849101.png" alt="image-20210222221849101" style="zoom:33%;" />

我们进一步考虑把TD代替MC引入到policy interaction中。TD 是给定了一个策略，然后我们去估计它的价值函数。接着我们要考虑怎么用 TD 这个框架来估计 Q-function。

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222222134763.png" alt="image-20210222222134763" style="zoom:33%;" />

我们先来看一下**Sarsa**算法。

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222223118614.png" alt="image-20210222223118614" style="zoom:33%;" />



<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222223312238.png" alt="image-20210222223312238" style="zoom: 33%;" />![image-20210222224836273](C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222224836273.png)



<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222225012635.png" alt="image-20210222225012635" style="zoom:33%;" />

所以每次都是单步更新

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222225716767.png" alt="image-20210222225716767" style="zoom:33%;" />

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222225122770.png" alt="image-20210222225122770" style="zoom:33%;" />

Sarsa 是一种 on-policy 策略。Sarsa 优化的是它实际执行的策略，它直接拿下一步会执行的 action 来去优化 Q  表格，所以 on-policy 在学习的过程中，只存在一种策略，它用一种策略去做 action 的选取，也用一种策略去做优化。所以 Sarsa  知道它下一步的动作有可能会跑到悬崖那边去，**所以它就会在优化它自己的策略的时候，会尽可能的离悬崖远一点。这样子就会保证说，它下一步哪怕是有随机动作，它也还是在安全区域内。**

而 off-policy 在学习的过程中，有两种不同的策略:

- 第一个策略是我们需要去学习的策略，即`target policy(目标策略)`，一般用 π\piπ 来表示，Target policy 就像是在后方指挥战术的一个军师，它可以根据自己的经验来学习最优的策略，不需要去和环境交互。
- 另外一个策略是探索环境的策略，即`behavior policy(行为策略)`，一般用 μ\muμ 来表示。μ\muμ 可以大胆地去探索到所有可能的轨迹，采集轨迹，采集数据，然后把采集到的数据喂给 target policy 去学习。**而且喂给目标策略的数据中并不需要 At+1A_{t+1}At+1 ，而 Sarsa 是要有 At+1A_{t+1}At+1 的。**Behavior policy 像是一个战士，可以在环境里面探索所有的动作、轨迹和经验，然后把这些经验交给目标策略去学习。**比如目标策略优化的时候，Q-learning 不会管你下一步去往哪里探索，它就只选收益最大的策略。**
- 
- ![image-20210222230211248](C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222230211248.png)

**Off-policy learning 有很多好处：**

- 我们可以利用 exploratory policy 来学到一个最佳的策略，学习效率高；
- 可以让我们学习其他 agent 的行为，模仿学习，学习人或者其他 agent 产生的轨迹；
- 重用老的策略产生的轨迹。探索过程需要很多计算资源，这样的话，可以节省资源。

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222230458585.png" alt="image-20210222230458585" style="zoom:33%;" />

主要的对比是很显然的

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222230649435.png" alt="image-20210222230649435" style="zoom:50%;" />

**Sarsa 和 Q-learning 的更新公式都是一样的，区别只在 target 计算的这一部分**

<img src="C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222230901503.png" alt="image-20210222230901503" style="zoom:33%;" />

![image-20210222230912783](C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222230912783.png)

- Sarsa 是一个典型的 **on-policy 策略**，它只用了一个 policy π\piπ 。如果 policy 采用 ε\varepsilonε-greedy 算法的话，**它需要兼顾探索，为了兼顾探索和利用，它训练的时候会显得有点胆小。**它在解决悬崖问题的时候，会尽可能地离悬崖边上远远的，确保说哪怕自己不小心探索了一点，也还是在安全区域内。**此外，因为采用的是 ε\varepsilonε-greedy 算法，策略会不断改变(ε\varepsilonε 会不断变小)，所以策略不稳定。**
- Q-learning 是一个典型的 **off-policy 的策略**，它有两种策略：target policy 和 behavior  policy。**它分离了目标策略跟行为策略**。Q-learning 就可以**大胆地用 behavior policy  去探索得到的经验轨迹**来去优化目标策略，从而更有可能去探索到最优的策略。Behavior policy 可以采用 ε\varepsilonε-greedy 算法，但 **target policy 采用的是 greedy 算法，直接根据 behavior policy 采集到的数据来采用最优策略，所以 Q-learning 不需要兼顾探索。**
- 比较 Q-learning 和 Sarsa 的更新公式可以发现，Sarsa 并没有选取最大值的 max 操作，因此，
  - Q-learning 是一个非常激进的算法，希望每一步都获得最大的利益；
  - 而 Sarsa 则相对非常保守，会选择一条相对安全的迭代路线。

![image-20210222231045365](C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222231045365.png)

![image-20210222231541290](C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222231541290.png)

![image-20210222231837458](C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222231837458.png)

![image-20210222231846365](C:\Users\Ced-3\AppData\Roaming\Typora\typora-user-images\image-20210222231846365.png)